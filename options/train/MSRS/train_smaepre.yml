# General settings
name: SMAEPretrain
model_type: SMAEPreModel
num_gpu: 1  # set num_gpu: 0 for CPU mode
manual_seed: 42

# Dataset and data loader settings
datasets:
  train:
    name: LLVIP
    type: SMAEPreDataset
    is_train: True
    is_RGB: False
    dataroot_source1: datasets/LLVIP/train/ir
    dataroot_source2: datasets/LLVIP/train/vi
    mask_path: datasets/LLVIP/train/max_mask
    ir_map_path: datasets/LLVIP/train/ir_mask
    vis_map_path: datasets/LLVIP/train/vis_mask
    img_size: 128
    num_worker_per_gpu: 12
    batch_size_per_gpu: 8
    dataset_enlarge_ratio: 1
    pin_memory: true
    prefetch_mode: cuda

# Network structures
network_g:
  type: SMAEPretrain
  inp_channels: 1
  dim: 64
  img_size: 128
  mask_ratio: 0.75
  encoder_configs: {
      'num_blocks': 4,
      'dim': 64,
      'num_heads': 8,
      'ffn_expansion_factor': 4,
      'bias': False,
      'LayerNorm_type': 'WithBias',
      'reduction': 8,
      'kernel_size': 3,
  }
  decoder_configs: {
      'num_blocks': 2,
      'dim': 64,
      'num_heads': 4,
      'ffn_expansion_factor': 2,
      'bias': False,
      'LayerNorm_type': 'WithBias',
      'reduction': 8,
      'kernel_size': 3,
  }

# Training settings
train:
  ema_decay: 0  # EMA not used
  clip_grad: False
  # Optimizers
  optim_g:
    type: AdamW
    lr: !!float 4e-4
    weight_decay: !!float 1e-2
    betas: [0.9, 0.99]

  # Scheduler: Cosine Annealing with Restarts
  scheduler:
    type: CosineAnnealingRestartLR
    periods: [250000, 250000]
    restart_weights: [1, 1]
    eta_min: !!float 1e-6

  total_iter: 500000
  warmup_iter: 5000  # Warmup iterations
  # Loss function for pretraining
  mask_pixel_opt:
    type: MSELoss
    loss_weight: 1.0
    reduction: mean

# Path settings
path:
  pretrain_network_g: ~  # No pretraining for now
  strict_load_g: true
  resume_state: ~  # Can resume training if needed

# Logging settings
logger:
  print_freq: 1000
  save_checkpoint_freq: !!float 1e4
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# Distributed training settings
dist_params:
  backend: nccl
  port: 29500
